{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f88640105d0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMSequenceLabler(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(LSTMSequenceLabler, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, dropout=0.2, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "#         self.loss = nn.BCELoss()\n",
    "        \n",
    "#         self.hidden = self.init_hidden()\n",
    "\n",
    "#     def init_hidden(self):\n",
    "#         return (torch.randn(2, 1, self.hidden_dim),\n",
    "#                 torch.randn(2, 1, self.hidden_dim))\n",
    "    \n",
    "    def get_loss(self, batch_x, batch_y):\n",
    "        word_embeddings = self.embedding(batch_x) #outputs 50, 15, <embedding_size>\n",
    "        lstm_out, _ = self.lstm(word_embeddings) #output 50, 15, <2 * hidden_size>\n",
    "        linear_out = self.linear(lstm_out) #output 50, 15, 1\n",
    "        sigmoid_out = self.sigmoid(linear_out) #output 50, 15, 1\n",
    "        prediction = sigmoid_out.view(max_len, -1)\n",
    "        \n",
    "        # custom loss function\n",
    "        # weighted binary cross entropy loss with 1 classifications are given higher priority\n",
    "        w = 3.0\n",
    "        loss = - (w * batch_y * torch.log(prediction) + (1-batch_y) * torch.log(1-prediction))\n",
    "        loss = torch.sum(loss)\n",
    "#         return self.loss(prediction, batch_y)\n",
    "        return loss\n",
    "    \n",
    "    def forward(self, batch_x):\n",
    "        word_embeddings = self.embedding(batch_x) #outputs 50, 15, <embedding_size>\n",
    "        lstm_out, _ = self.lstm(word_embeddings) #output 50, 15, <2 * hidden_size>\n",
    "        linear_out = self.linear(lstm_out) #output 50, 15, 1\n",
    "        sigmoid_out = self.sigmoid(linear_out) #output 50, 15, 1\n",
    "        prediction = sigmoid_out.view(max_len, -1)\n",
    "        \n",
    "        prediction[prediction >= 0.5] = 1\n",
    "        prediction[prediction < 0.5] = 0\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1 11689.6832848\n",
      "True pos 0.0\n",
      "Tot pos predicted 0.0\n",
      "Loss at epoch 21 4127.71922493\n",
      "True pos 0.0\n",
      "Tot pos predicted 0.0\n",
      "Loss at epoch 41 2659.08716011\n",
      "True pos 16.0\n",
      "Tot pos predicted 26.0\n",
      "Loss at epoch 61 1195.99258018\n",
      "True pos 39.0\n",
      "Tot pos predicted 54.0\n",
      "Loss at epoch 81 507.784154415\n",
      "True pos 41.0\n",
      "Tot pos predicted 49.0\n",
      "Loss at epoch 101 270.818754077\n",
      "True pos 41.0\n",
      "Tot pos predicted 49.0\n",
      "Loss at epoch 121 147.713740945\n",
      "True pos 41.0\n",
      "Tot pos predicted 44.0\n",
      "Loss at epoch 141 93.9536977112\n",
      "True pos 41.0\n",
      "Tot pos predicted 45.0\n",
      "Loss at epoch 161 169.856117032\n",
      "True pos 41.0\n",
      "Tot pos predicted 42.0\n",
      "Loss at epoch 181 41.1793210134\n",
      "True pos 41.0\n",
      "Tot pos predicted 43.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "PADDING = \"<PAD>\"\n",
    "# max sentence size\n",
    "max_len = 50\n",
    "\n",
    "EMBEDDING_DIM = 5\n",
    "HIDDEN_DIM = 4\n",
    "\n",
    "batch_size = 15\n",
    "\n",
    "train_file_x = 'data/restaurants_trial_x.txt'\n",
    "train_file_y = 'data/restaurants_trial_y.txt'\n",
    "\n",
    "def make_list(path):\n",
    "    f = open(path)\n",
    "    return [x.split() for x in f]\n",
    "\n",
    "train_x = make_list(train_file_x)\n",
    "# print('max len', max(map(len, train_x)))\n",
    "train_y = make_list(train_file_y)\n",
    "# training_data = list(zip(train_x, train_y))\n",
    "\n",
    "# add padding\n",
    "for i in xrange(len(train_x)):\n",
    "    for _ in xrange(max_len-len(train_x[i])):\n",
    "        train_x[i].append('<PAD>')\n",
    "        train_y[i].append('0')\n",
    "\n",
    "word_to_ix = {}\n",
    "for sentence in train_x:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "            \n",
    "# make train data tensors\n",
    "for i in xrange(len(train_x)):\n",
    "    train_x[i] = map(lambda x: word_to_ix[x], train_x[i])\n",
    "    train_y[i] = map(int, train_y[i])\n",
    "\n",
    "train_x = torch.LongTensor(train_x)\n",
    "train_y = torch.Tensor(train_y)\n",
    "\n",
    "training_data = zip(train_x, train_y)\n",
    "\n",
    "\n",
    "# embedding = nn.Embedding(len(word_to_ix), EMBEDDING_DIM)\n",
    "# lstm = nn.LSTM(input_size=EMBEDDING_DIM, hidden_size=HIDDEN_DIM, num_layers=2, dropout=0.2, bidirectional=True)\n",
    "# linear = nn.Linear(HIDDEN_DIM * 2, 1)\n",
    "# sigmoid = nn.Sigmoid()\n",
    "\n",
    "model = LSTMSequenceLabler(len(word_to_ix), EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "for epoch in range(200):\n",
    "    batch_id = 0\n",
    "    start = 0\n",
    "    end = 0\n",
    "    \n",
    "    loss_val = 0\n",
    "    \n",
    "    while(batch_id < int(math.ceil(len(training_data)/batch_size))):\n",
    "        start = batch_id * batch_size\n",
    "        end = min(start + batch_size, len(training_data))\n",
    "        \n",
    "        batch_x = train_x[start:end].permute(dims=(1,0)) #shape = 50 (max_len), 15 (batch_size)\n",
    "        batch_y = train_y[start:end].permute(dims=(1,0))\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss = model.get_loss(batch_x, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_val += loss.tolist() * (end - start)\n",
    "        batch_id += 1\n",
    "    \n",
    "    \n",
    "    if epoch%20==0:\n",
    "        print(\"Loss at epoch %d\" % (epoch + 1), loss_val)\n",
    "        # check training accuracy in each 20 epochs\n",
    "        \n",
    "        prediction = model(train_x.permute(dims=(1,0)))\n",
    "        target = train_y.permute(dims=(1,0))\n",
    "        \n",
    "        true_pos = torch.sum(target * prediction).tolist()\n",
    "        print(\"True pos\", true_pos)\n",
    "        tot_pos = torch.sum(target).tolist()\n",
    "        tot_predicted = torch.sum(prediction).tolist()\n",
    "        print(\"Tot pos predicted\", tot_predicted)\n",
    "        false_pos = torch.sum(prediction).tolist() - true_pos\n",
    "        \n",
    "#         if tot_predicted!=0 and tot_pos!=0:\n",
    "#             print(\"Precision\", true_pos/tot_predicted)\n",
    "#             print(\"Recall\", true_pos/tot_pos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
