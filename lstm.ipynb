{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3d5c0325b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMSequenceLabler(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(LSTMSequenceLabler, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, dropout=0.2, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "#         self.loss = nn.BCELoss()\n",
    "        \n",
    "#         self.hidden = self.init_hidden()\n",
    "\n",
    "#     def init_hidden(self):\n",
    "#         return (torch.randn(2, 1, self.hidden_dim),\n",
    "#                 torch.randn(2, 1, self.hidden_dim))\n",
    "    \n",
    "    def get_loss(self, batch_x, batch_y):\n",
    "        word_embeddings = self.embedding(batch_x) #outputs 50, 15, <embedding_size>\n",
    "        lstm_out, _ = self.lstm(word_embeddings) #output 50, 15, <2 * hidden_size>\n",
    "        linear_out = self.linear(lstm_out) #output 50, 15, 1\n",
    "        sigmoid_out = self.sigmoid(linear_out) #output 50, 15, 1\n",
    "        prediction = sigmoid_out.view(max_len, -1)\n",
    "        \n",
    "        # custom loss function\n",
    "        # weighted binary cross entropy loss with 1 classifications are given higher priority\n",
    "        w = 3.0\n",
    "        loss = - (w * batch_y * torch.log(prediction) + (1-batch_y) * torch.log(1-prediction))\n",
    "        loss = torch.sum(loss)\n",
    "#         return self.loss(prediction, batch_y)\n",
    "        return loss\n",
    "    \n",
    "    def forward(self, batch_x):\n",
    "        word_embeddings = self.embedding(batch_x) #outputs 50, 15, <embedding_size>\n",
    "        lstm_out, _ = self.lstm(word_embeddings) #output 50, 15, <2 * hidden_size>\n",
    "        linear_out = self.linear(lstm_out) #output 50, 15, 1\n",
    "        sigmoid_out = self.sigmoid(linear_out) #output 50, 15, 1\n",
    "        prediction = sigmoid_out.view(max_len, -1)\n",
    "        \n",
    "        prediction[prediction >= 0.5] = 1\n",
    "        prediction[prediction < 0.5] = 0\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Loss at epoch 0 162716.599823\n",
      "-----------------------------------------\n",
      "Training Set Precision 0.6875\n",
      "Training Set Recall 0.373303167421\n",
      "-----------------------------------------\n",
      "Test Set Precision 0.728915662651\n",
      "Test Set Recall 0.306329113924\n",
      "==========================================\n",
      "Saving model...\n",
      "==========================================\n",
      "Loss at epoch 20 17657.7427855\n",
      "-----------------------------------------\n",
      "Training Set Precision 0.835455435847\n",
      "Training Set Recall 0.964932126697\n",
      "-----------------------------------------\n",
      "Test Set Precision 0.642857142857\n",
      "Test Set Recall 0.637974683544\n",
      "==========================================\n",
      "Saving model...\n",
      "==========================================\n",
      "Loss at epoch 40 8589.32146096\n",
      "-----------------------------------------\n",
      "Training Set Precision 0.89751552795\n",
      "Training Set Recall 0.980769230769\n",
      "-----------------------------------------\n",
      "Test Set Precision 0.667597765363\n",
      "Test Set Recall 0.605063291139\n",
      "==========================================\n",
      "Saving model...\n",
      "==========================================\n",
      "Loss at epoch 60 3341.61693266\n",
      "-----------------------------------------\n",
      "Training Set Precision 0.97552836485\n",
      "Training Set Recall 0.992081447964\n",
      "-----------------------------------------\n",
      "Test Set Precision 0.757575757576\n",
      "Test Set Recall 0.569620253165\n",
      "==========================================\n",
      "Saving model...\n",
      "==========================================\n",
      "Loss at epoch 80 2789.96902278\n",
      "-----------------------------------------\n",
      "Training Set Precision 0.977578475336\n",
      "Training Set Recall 0.986425339367\n",
      "-----------------------------------------\n",
      "Test Set Precision 0.723549488055\n",
      "Test Set Recall 0.536708860759\n",
      "==========================================\n",
      "Saving model...\n",
      "==========================================\n",
      "Loss at epoch 100 3124.13233793\n",
      "-----------------------------------------\n",
      "Training Set Precision 0.988713318284\n",
      "Training Set Recall 0.990950226244\n",
      "-----------------------------------------\n",
      "Test Set Precision 0.759398496241\n",
      "Test Set Recall 0.511392405063\n",
      "==========================================\n",
      "Saving model...\n",
      "==========================================\n",
      "Loss at epoch 120 2016.39760281\n",
      "-----------------------------------------\n",
      "Training Set Precision 0.981111111111\n",
      "Training Set Recall 0.998868778281\n",
      "-----------------------------------------\n",
      "Test Set Precision 0.69552238806\n",
      "Test Set Recall 0.589873417722\n",
      "==========================================\n",
      "Saving model...\n",
      "==========================================\n",
      "Loss at epoch 140 1695.4063994\n",
      "-----------------------------------------\n",
      "Training Set Precision 0.989887640449\n",
      "Training Set Recall 0.996606334842\n",
      "-----------------------------------------\n",
      "Test Set Precision 0.746428571429\n",
      "Test Set Recall 0.529113924051\n",
      "==========================================\n",
      "Saving model...\n",
      "==========================================\n",
      "Loss at epoch 160 1128.3511091\n",
      "-----------------------------------------\n",
      "Training Set Precision 0.989853438557\n",
      "Training Set Recall 0.993212669683\n",
      "-----------------------------------------\n",
      "Test Set Precision 0.768656716418\n",
      "Test Set Recall 0.521518987342\n",
      "==========================================\n",
      "Saving model...\n",
      "==========================================\n",
      "Loss at epoch 180 960.642609693\n",
      "-----------------------------------------\n",
      "Training Set Precision 0.995485327314\n",
      "Training Set Recall 0.997737556561\n",
      "-----------------------------------------\n",
      "Test Set Precision 0.75\n",
      "Test Set Recall 0.53164556962\n",
      "==========================================\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "PADDING = \"<PAD>\"\n",
    "# max sentence size\n",
    "max_len = 68\n",
    "\n",
    "EMBEDDING_DIM = 20\n",
    "HIDDEN_DIM = 8\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "train_file_x = 'data/restaurants_train_x.txt'\n",
    "train_file_y = 'data/restaurants_train_y.txt'\n",
    "\n",
    "test_file_x = 'data/restaurants_test_x.txt'\n",
    "test_file_y = 'data/restaurants_test_y.txt'\n",
    "\n",
    "def make_list(path):\n",
    "    f = open(path)\n",
    "    return [x.split() for x in f]\n",
    "\n",
    "train_x = make_list(train_file_x)\n",
    "# print('max len', max(map(len, train_x)))\n",
    "train_y = make_list(train_file_y)\n",
    "# training_data = list(zip(train_x, train_y))\n",
    "test_x = make_list(test_file_x)\n",
    "test_y = make_list(test_file_y)\n",
    "\n",
    "# print('max len', max(map(len, test_x)))\n",
    "\n",
    "# add padding\n",
    "for i in xrange(len(train_x)):\n",
    "    for _ in xrange(max_len-len(train_x[i])):\n",
    "        train_x[i].append('<PAD>')\n",
    "        train_y[i].append('0')\n",
    "\n",
    "for i in xrange(len(test_x)):\n",
    "    for _ in xrange(max_len-len(test_x[i])):\n",
    "        test_x[i].append('<PAD>')\n",
    "        test_y[i].append('0')\n",
    "\n",
    "word_to_ix = {}\n",
    "for sentence in train_x:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "# for unknown words, only in testing, randomly initialized\n",
    "word_to_ix['<unk>'] = len(word_to_ix)\n",
    "\n",
    "# make train data tensors\n",
    "for i in xrange(len(train_x)):\n",
    "    train_x[i] = map(lambda x: word_to_ix[x], train_x[i])\n",
    "    train_y[i] = map(int, train_y[i])\n",
    "    \n",
    "for i in xrange(len(test_x)):\n",
    "    test_x[i] = map(lambda x: word_to_ix[x] if x in word_to_ix else word_to_ix['<unk>'], test_x[i])\n",
    "    test_y[i] = map(int, test_y[i])\n",
    "\n",
    "train_x = torch.LongTensor(train_x)\n",
    "train_y = torch.Tensor(train_y)\n",
    "test_x = torch.LongTensor(test_x)\n",
    "test_y = torch.Tensor(test_y)\n",
    "\n",
    "training_data = zip(train_x, train_y)\n",
    "\n",
    "model = LSTMSequenceLabler(len(word_to_ix), EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "for epoch in range(200):\n",
    "    batch_id = 0\n",
    "    start = 0\n",
    "    end = 0\n",
    "    \n",
    "    loss_val = 0\n",
    "    \n",
    "    while(batch_id < int(math.ceil(len(training_data)/batch_size))):\n",
    "        start = batch_id * batch_size\n",
    "        end = min(start + batch_size, len(training_data))\n",
    "        \n",
    "        batch_x = train_x[start:end].permute(dims=(1,0)) #shape = 68 (max_len), 15 (batch_size)\n",
    "        batch_y = train_y[start:end].permute(dims=(1,0))\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss = model.get_loss(batch_x, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_val += loss.tolist() * (end - start)\n",
    "        batch_id += 1\n",
    "    \n",
    "    print('Epoch %d/20' % (epoch%20))\n",
    "    if epoch%20==0:\n",
    "        print(\"==========================================\")\n",
    "        print(\"Loss at epoch %d\" % epoch, loss_val)\n",
    "        print(\"-----------------------------------------\")\n",
    "        # check training accuracy in each 20 epochs\n",
    "        \n",
    "        ## Training\n",
    "        prediction = model(train_x.permute(dims=(1,0)))\n",
    "        target = train_y.permute(dims=(1,0))\n",
    "        true_pos = torch.sum(target * prediction).tolist()\n",
    "        tot_pos = torch.sum(target).tolist()\n",
    "        tot_predicted = torch.sum(prediction).tolist()\n",
    "        false_pos = torch.sum(prediction).tolist() - true_pos\n",
    "        \n",
    "        if tot_predicted!=0 and tot_pos!=0:\n",
    "            print(\"Training Set Precision\", true_pos/tot_predicted)\n",
    "            print(\"Training Set Recall\", true_pos/tot_pos)\n",
    "        \n",
    "        print(\"-----------------------------------------\")\n",
    "        ## Testing\n",
    "        prediction = model(test_x.permute(dims=(1,0)))\n",
    "        target = test_y.permute(dims=(1,0))\n",
    "        true_pos = torch.sum(target * prediction).tolist()\n",
    "        tot_pos = torch.sum(target).tolist()\n",
    "        tot_predicted = torch.sum(prediction).tolist()\n",
    "        false_pos = torch.sum(prediction).tolist() - true_pos\n",
    "        \n",
    "        if tot_predicted!=0 and tot_pos!=0:\n",
    "            print(\"Test Set Precision\", true_pos/tot_predicted)\n",
    "            print(\"Test Set Recall\", true_pos/tot_pos)\n",
    "            \n",
    "        print(\"==========================================\")\n",
    "        \n",
    "        print(\"Saving model...\")\n",
    "        torch.save(model.state_dict(), 'lstm_models/model_' + str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), 'first_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model = LSTMSequenceLabler(len(word_to_ix), EMBEDDING_DIM, HIDDEN_DIM)\n",
    "model.load_state_dict(torch.load('first_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], [1], [0], [0], [0], [0], [1], [0], [0], [0]]\n"
     ]
    }
   ],
   "source": [
    "import nltk.tokenize\n",
    "\n",
    "sentence = nltk.tokenize.word_tokenize(\"Their service was terrible but the atmosphere was bad !\")\n",
    "pre_size = len(sentence)\n",
    "\n",
    "for _ in xrange(max_len-len(sentence)):\n",
    "    sentence.append('<PAD>')\n",
    "        \n",
    "sentence = torch.LongTensor(map(lambda x: word_to_ix[x] if x in word_to_ix else word_to_ix['<unk>'], sentence))\n",
    "\n",
    "print(model(sentence.view(max_len, 1)).tolist()[:pre_size])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
