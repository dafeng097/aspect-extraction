{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f88640105d0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMSequenceLabler(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(LSTMSequenceLabler, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, dropout=0.2, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "#         self.loss = nn.BCELoss()\n",
    "        \n",
    "#         self.hidden = self.init_hidden()\n",
    "\n",
    "#     def init_hidden(self):\n",
    "#         return (torch.randn(2, 1, self.hidden_dim),\n",
    "#                 torch.randn(2, 1, self.hidden_dim))\n",
    "    \n",
    "    def get_loss(self, batch_x, batch_y):\n",
    "        word_embeddings = self.embedding(batch_x) #outputs 50, 15, <embedding_size>\n",
    "        lstm_out, _ = self.lstm(word_embeddings) #output 50, 15, <2 * hidden_size>\n",
    "        linear_out = self.linear(lstm_out) #output 50, 15, 1\n",
    "        sigmoid_out = self.sigmoid(linear_out) #output 50, 15, 1\n",
    "        prediction = sigmoid_out.view(max_len, -1)\n",
    "        \n",
    "        # custom loss function\n",
    "        # weighted binary cross entropy loss with 1 classifications are given higher priority\n",
    "        w = 3.0\n",
    "        loss = - (w * batch_y * torch.log(prediction) + (1-batch_y) * torch.log(1-prediction))\n",
    "        loss = torch.sum(loss)\n",
    "#         return self.loss(prediction, batch_y)\n",
    "        return loss\n",
    "    \n",
    "    def forward(self, batch_x):\n",
    "        word_embeddings = self.embedding(batch_x) #outputs 50, 15, <embedding_size>\n",
    "        lstm_out, _ = self.lstm(word_embeddings) #output 50, 15, <2 * hidden_size>\n",
    "        linear_out = self.linear(lstm_out) #output 50, 15, 1\n",
    "        sigmoid_out = self.sigmoid(linear_out) #output 50, 15, 1\n",
    "        prediction = sigmoid_out.view(max_len, -1)\n",
    "        \n",
    "        prediction[prediction >= 0.5] = 1\n",
    "        prediction[prediction < 0.5] = 0\n",
    "        \n",
    "        return prediction.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1 136630.188667\n",
      "True pos 0\n",
      "Tot pos predicted 0\n",
      "Loss at epoch 21 24856.8224063\n",
      "True pos 274\n",
      "Tot pos predicted 429\n",
      "Precision 0.638694638695\n",
      "Recall 0.693670886076\n",
      "Loss at epoch 41 16806.1950073\n",
      "True pos 282\n",
      "Tot pos predicted 438\n",
      "Precision 0.643835616438\n",
      "Recall 0.713924050633\n",
      "Loss at epoch 61 13068.8999619\n",
      "True pos 279\n",
      "Tot pos predicted 421\n",
      "Precision 0.66270783848\n",
      "Recall 0.706329113924\n",
      "Loss at epoch 81 10967.3413696\n",
      "True pos 287\n",
      "Tot pos predicted 412\n",
      "Precision 0.696601941748\n",
      "Recall 0.726582278481\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "PADDING = \"<PAD>\"\n",
    "# max sentence size\n",
    "max_len = 68\n",
    "\n",
    "EMBEDDING_DIM = 5\n",
    "HIDDEN_DIM = 4\n",
    "\n",
    "batch_size = 15\n",
    "\n",
    "train_file_x = 'data/restaurants_train_x.txt'\n",
    "train_file_y = 'data/restaurants_train_y.txt'\n",
    "\n",
    "test_file_x = 'data/restaurants_test_x.txt'\n",
    "test_file_y = 'data/restaurants_test_y.txt'\n",
    "\n",
    "def make_list(path):\n",
    "    f = open(path)\n",
    "    return [x.split() for x in f]\n",
    "\n",
    "train_x = make_list(train_file_x)\n",
    "# print('max len', max(map(len, train_x)))\n",
    "train_y = make_list(train_file_y)\n",
    "# training_data = list(zip(train_x, train_y))\n",
    "test_x = make_list(test_file_x)\n",
    "test_y = make_list(test_file_y)\n",
    "\n",
    "# print('max len', max(map(len, test_x)))\n",
    "\n",
    "# add padding\n",
    "for i in xrange(len(train_x)):\n",
    "    for _ in xrange(max_len-len(train_x[i])):\n",
    "        train_x[i].append('<PAD>')\n",
    "        train_y[i].append('0')\n",
    "\n",
    "for i in xrange(len(test_x)):\n",
    "    for _ in xrange(max_len-len(test_x[i])):\n",
    "        test_x[i].append('<PAD>')\n",
    "        test_y[i].append('0')\n",
    "\n",
    "word_to_ix = {}\n",
    "for sentence in train_x:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "# for unknown words, only in testing, randomly initialized\n",
    "word_to_ix['<unk>'] = len(word_to_ix)\n",
    "\n",
    "# make train data tensors\n",
    "for i in xrange(len(train_x)):\n",
    "    train_x[i] = map(lambda x: word_to_ix[x], train_x[i])\n",
    "    train_y[i] = map(int, train_y[i])\n",
    "    \n",
    "for i in xrange(len(test_x)):\n",
    "    test_x[i] = map(lambda x: word_to_ix[x] if x in word_to_ix else word_to_ix['<unk>'], test_x[i])\n",
    "    test_y[i] = map(int, test_y[i])\n",
    "\n",
    "train_x = torch.LongTensor(train_x)\n",
    "train_y = torch.Tensor(train_y)\n",
    "test_x = torch.LongTensor(test_x)\n",
    "test_y = torch.LongTensor(test_y)\n",
    "\n",
    "training_data = zip(train_x, train_y)\n",
    "\n",
    "model = LSTMSequenceLabler(len(word_to_ix), EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "for epoch in range(200):\n",
    "    batch_id = 0\n",
    "    start = 0\n",
    "    end = 0\n",
    "    \n",
    "    loss_val = 0\n",
    "    \n",
    "    while(batch_id < int(math.ceil(len(training_data)/batch_size))):\n",
    "        start = batch_id * batch_size\n",
    "        end = min(start + batch_size, len(training_data))\n",
    "        \n",
    "        batch_x = train_x[start:end].permute(dims=(1,0)) #shape = 50 (max_len), 15 (batch_size)\n",
    "        batch_y = train_y[start:end].permute(dims=(1,0))\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss = model.get_loss(batch_x, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_val += loss.tolist() * (end - start)\n",
    "        batch_id += 1\n",
    "    \n",
    "    \n",
    "    if epoch%20==0:\n",
    "        print(\"==========================================\")\n",
    "        print(\"Loss at epoch %d\" % epoch, loss_val)\n",
    "        # check training accuracy in each 20 epochs\n",
    "        \n",
    "        ## Training\n",
    "        prediction = model(train_x.permute(dims=(1,0)))\n",
    "        target = train_y.permute(dims=(1,0))\n",
    "        true_pos = torch.sum(target * prediction).tolist()\n",
    "        tot_pos = torch.sum(target).tolist()\n",
    "        tot_predicted = torch.sum(prediction).tolist()\n",
    "        false_pos = torch.sum(prediction).tolist() - true_pos\n",
    "        \n",
    "        if tot_predicted!=0 and tot_pos!=0:\n",
    "            print(\"Training Set Precision\", true_pos/tot_predicted)\n",
    "            print(\"Training Set Recall\", true_pos/tot_pos)\n",
    "        \n",
    "        print(\"-----------------------------------------\")\n",
    "        ## Testing\n",
    "        prediction = model(test_x.permute(dims=(1,0)))\n",
    "        target = test_y.permute(dims=(1,0))\n",
    "        \n",
    "        true_pos = torch.sum(target * prediction).tolist()\n",
    "        tot_pos = torch.sum(target).tolist()\n",
    "        tot_predicted = torch.sum(prediction).tolist()\n",
    "        false_pos = torch.sum(prediction).tolist() - true_pos\n",
    "        \n",
    "        if tot_predicted!=0 and tot_pos!=0:\n",
    "            print(\"Test Set Precision\", true_pos/tot_predicted)\n",
    "            print(\"Test Set Recall\", true_pos/tot_pos)\n",
    "            \n",
    "        print(\"==========================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
